<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://panson.top</id>
    <title>Panson</title>
    <updated>2025-11-01T13:59:17.958Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://panson.top"/>
    <link rel="self" href="https://panson.top/atom.xml"/>
    <subtitle>清风拂山岗，明月照大江。</subtitle>
    <logo>https://panson.top/images/avatar.png</logo>
    <icon>https://panson.top/favicon.ico</icon>
    <rights>All rights reserved 2025, Panson</rights>
    <entry>
        <title type="html"><![CDATA[关于本站]]></title>
        <id>https://panson.top/post/zhi-ding-tie/</id>
        <link href="https://panson.top/post/zhi-ding-tie/">
        </link>
        <updated>2100-10-14T02:48:07.000Z</updated>
        <summary type="html"><![CDATA[<p><em>分享自己的所见所得，后端技术、供应链、任务调度（AI阅片任务调度、渲染任务调度）、摄影相关。</em></p>
<hr>
<h2 id="一-关于本站">一、关于本站</h2>
<ol>
<li>
<p>博客基于 Github + Gridea，由于众所周知的原因，科学上网后才能流畅阅读。</p>
</li>
<li>
<p>博客使用的 Featured Image 和正文首部插图来源: <a href="https://unsplash.com/">unsplash</a><br>
、<a href="https://film-grab.com/">FILMGRAB</a>、豆瓣、 <a href="https://screenmusings.org/">screenmusings</a>、<a href="https://www.ysjf.com/material">影视飓风</a>，大多数图片也需要科学上网才能正常加载。</p>
</li>
<li>
<p>当然，也有部分是我自己的“摄影作品”。</p>
</li>
</ol>
<h2 id="二-供应链">二、供应链</h2>
<p>目前做的是供应链领域的后端开发，看过一些书和资料，结合日常工作内容，写了一个小系列。<br>
具体可以看：<a href="https://panson.top/post/supply-chain/">https://panson.top/post/supply-chain/</a>，也在这个站点下。</p>
<h2 id="三-任务调度">三、任务调度</h2>
<p>更早之前的两份工作内容都和任务调度有关，主要是关于 AI 任务调度和渲染任务调度。当然这种“任务”</p>
]]></summary>
        <content type="html"><![CDATA[<p><em>分享自己的所见所得，后端技术、供应链、任务调度（AI阅片任务调度、渲染任务调度）、摄影相关。</em></p>
<hr>
<h2 id="一-关于本站">一、关于本站</h2>
<ol>
<li>
<p>博客基于 Github + Gridea，由于众所周知的原因，科学上网后才能流畅阅读。</p>
</li>
<li>
<p>博客使用的 Featured Image 和正文首部插图来源: <a href="https://unsplash.com/">unsplash</a><br>
、<a href="https://film-grab.com/">FILMGRAB</a>、豆瓣、 <a href="https://screenmusings.org/">screenmusings</a>、<a href="https://www.ysjf.com/material">影视飓风</a>，大多数图片也需要科学上网才能正常加载。</p>
</li>
<li>
<p>当然，也有部分是我自己的“摄影作品”。</p>
</li>
</ol>
<h2 id="二-供应链">二、供应链</h2>
<p>目前做的是供应链领域的后端开发，看过一些书和资料，结合日常工作内容，写了一个小系列。<br>
具体可以看：<a href="https://panson.top/post/supply-chain/">https://panson.top/post/supply-chain/</a>，也在这个站点下。</p>
<h2 id="三-任务调度">三、任务调度</h2>
<p>更早之前的两份工作内容都和任务调度有关，主要是关于 AI 任务调度和渲染任务调度。当然这种“任务”</p>
<!-- more -->
<p>和后端开发中的任务调度框架不太一样，类似 XXL-JOB 这种，我一般理解为定时任务，不带业务属性，但之前做的 AI 任务调度和渲染任务调度则更偏向于业务。下面简单介绍一下我参与做过的内容（其实本来应该写一个开源脚手架的，最近沉迷摄影，一直没时间去回想这些东西，时间过去有点久了，细节忘记了~提醒我及时记录）。</p>
<h3 id="1-ai-任务调度系统">1. AI 任务调度系统</h3>
<p>属于医疗领域，核心是为了将医院、体检机构拍摄的片子，借助 AI 智能阅片，并输出诊断结果，辅助治疗。</p>
<ul>
<li>数据来源：体检机构、大中小医院的片子</li>
<li>文件系统：上传的片子预处理，存储文件、预处理的数据</li>
<li>任务调度系统：
<ul>
<li>Master 将处理好的片子，做任务分发到各个 Slave 机器上</li>
<li>Slave 上的算法服务会将任务做AI 分析，存储分析结果</li>
</ul>
</li>
<li>其他的还有阅片系统、SSO 平台、前置机之类的，还有单机平台、一体机之类的，非调度核心系统。</li>
</ul>
<h3 id="2-渲染任务调度">2. 渲染任务调度</h3>
<p>核心逻辑其实与上面讲的差不多，不过业务逻辑更加复杂，链路也更加长一些，数据量的话日均有千万级别。<br>
负责的东西主要包括以下几个部分：</p>
<ul>
<li>渲染弹窗相关：渲染配置、分辨率体系、渲染券相关</li>
<li>中台相关业务：国际版 CooHom、灯光动画、遮罩渲染相关能力的开发、对象存储迁移（阿里OSS -&gt; 腾讯 COS）等等</li>
<li>渲染任务回归平台的开发</li>
</ul>
<h2 id="四-摄影">四、摄影</h2>
<p>单独建了一个网站，买了 10 年的域名：www.timelesslens.site。<br>
<img src="https://raw.githubusercontent.com/PansonPanson/blog-pic-2023/main/2023/202311291106687.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[一次 JVM “神优化” 导致的日志丢失问题]]></title>
        <id>https://panson.top/post/yi-ci-jvm-shen-you-hua-dao-zhi-de-ri-zhi-diu-shi-wen-ti/</id>
        <link href="https://panson.top/post/yi-ci-jvm-shen-you-hua-dao-zhi-de-ri-zhi-diu-shi-wen-ti/">
        </link>
        <updated>2025-10-30T05:56:13.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="一-问题发生">一、问题发生</h2>
<p>今天同事私聊我，问了我一个问题，他说在一个美国的海外仓项目中发现一个奇怪的报错日志：只有error 日志信息和一个 NPE 日常，但是没有打印出具体的堆栈。</p>
<p>他问我有没有遇到过。</p>
<p>我心里第一个想法：是不是打印日志的地方写得不规范，只把 Exception 的 message 打印了，类似这样：</p>
]]></summary>
        <content type="html"><![CDATA[<h2 id="一-问题发生">一、问题发生</h2>
<p>今天同事私聊我，问了我一个问题，他说在一个美国的海外仓项目中发现一个奇怪的报错日志：只有error 日志信息和一个 NPE 日常，但是没有打印出具体的堆栈。</p>
<p>他问我有没有遇到过。</p>
<p>我心里第一个想法：是不是打印日志的地方写得不规范，只把 Exception 的 message 打印了，类似这样：</p>
<!-- more -->
<pre><code class="language-java">log.error(&quot;…… &quot;, exception.getMessage());
</code></pre>
<p>去看了一下代码，发现代码中其实是完整打印的：</p>
<pre><code class="language-java">try {
    // 执行调度逻辑：方法内部调用算法服务，分配库存
} catch (Exception exception) {
     log.error(&quot;……&quot;,  exception);
} finally {
     //
}
</code></pre>
<h2 id="二-求助-ai">二、求助 AI</h2>
<p>面对这个反常的现象，我一开始也向我的好帮手——AI 提问了。</p>
<p>我尝试了各种姿势，并给了一些上下文，去问 AI，得到了一些排查方向：<br>
​1. 确认日志框架​<br>
确认项目使用的是 Logback, Log4j2 还是其他日志框架。<br>
​2. 检查日志级别​<br>
确保记录异常时使用的级别（如 ERROR）在配置中是启用的。<br>
​3. 检查日志配置​<br>
核对配置文件（如 logback.xml, log4j2.xml），看输出目的地（控制台、文件）是否正确。<br>
​4. 验证日志实现​<br>
在 catch块中增加简单输出（如 System.out.println），确认代码执行路径。<br>
……</p>
<p>但似乎没找到我想要的答案。</p>
<h2 id="三-拨云见日十年前的老帖子">三、拨云见日：十年前的“老帖子”</h2>
<p>我决定重新梳理所有上下文，包括业务的调用频率、JVM 的运行环境等等。这个定时器是高频运行的，且每次都会在<strong>同一个地方</strong>报 <strong>NPE</strong>。</p>
<p>我抱着试一试的心态，在 Google/知乎上搜索一些“Java 异常 堆栈 丢失 性能优化”之类的关键词。结果，R 大在 <strong>十年前</strong> 的回答，瞬间击碎了所有的迷雾！</p>
<blockquote>
<p><strong>帖子地址：</strong> <a href="https://www.zhihu.com/question/21405047/answer/45055055">重载 Throwable.fillInStackTrace() 方法以提高Java性能这样的做法对吗？ - RednaxelaFX的回答 - 知乎</a></p>
</blockquote>
<p>R 大的回答里提到了一个 <strong>HotSpot VM</strong> 的“神优化”——<strong>fast throw</strong>！</p>
<blockquote>
<p>HotSpot VM 有个许多人觉得“匪夷所思”的优化，叫做 <strong>fast throw</strong>：有些特定的隐式异常类型（<code>NullPointerException</code>、<code>ArithmeticException</code>（ <code>/ 0</code>）之类）如果在代码里某个特定位置被抛出过多次的话，HotSpot Server Compiler（C2）会透明地决定用 <strong>fast throw</strong> 来优化这个抛出异常的地方——直接抛出一个事先分配好的、类型匹配的异常对象。这个对象的 <code>message</code> 和 <strong>stack trace 都被清空</strong>。</p>
</blockquote>
<p><strong>划重点：</strong> <strong><code>message</code> 和 <code>stack trace 都被清空</code>！</strong></p>
<p><strong>简直是醍醐灌顶！</strong></p>
<p>这种优化的目的是：**抛出这个异常的速度是非常快，不但不用额外分配内存，而且也不用爬栈；**但反面就是：<strong>可能正好是需要知道哪里出问题的时候看不到 stack trace 了。</strong></p>
<p>这不就是我的问题吗？！</p>
<p>顺着这个思路，看了一下这个定时器最早的一次报错，果然是带完整的堆栈信息的。<br>
结合我们的代码，至此可以破案了：获取库位信息并遍历的代码，它被<strong>高频</strong>调用，而且由于数据问题，它一直在报 <code>NPE</code>。</p>
<p>JVM 的 C2 编译器一看：“呦，指这行代码天天抛一样的异常，怪累的，我帮他优化一下吧！” 于是，它启动了 <strong>fast throw</strong> 机制，直接抛出预先分配好的、没有堆栈的 <code>NPE</code> 对象，大大提升了抛出异常的速度，但同时也“贴心”地清空了我的日志堆栈！</p>
<h2 id="四-反思">四、反思</h2>
<p>这事儿说到底，是因为我们代码里的 <strong>NPE Bug</strong> 成了 JVM 优化的“燃料”。</p>
<p>当然，知道原因后，我们首先要做的肯定是修复那个导致 NPE 的 Bug。</p>
<p>但从 JVM 层面，如果你确实需要知道高频异常的堆栈，你可以通过一个 VM 参数来<strong>禁用</strong>这个优化：</p>
<pre><code class="language-bash">-XX:-OmitStackTraceInFastThrow
</code></pre>
<p>不过，这个参数的意义是“以性能为代价，换取更完整的堆栈信息”，所以，<strong>更正确的姿势，永远是写出健壮的代码，避免高频抛出隐式异常！</strong></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[提示词工程学习记录]]></title>
        <id>https://panson.top/post/ti-shi-ci-gong-cheng-xue-xi-ji-lu/</id>
        <link href="https://panson.top/post/ti-shi-ci-gong-cheng-xue-xi-ji-lu/">
        </link>
        <updated>2025-06-28T13:09:01.000Z</updated>
        <content type="html"><![CDATA[<h2 id="一-大语言模型设置">一、大语言模型设置</h2>
<p><strong>Temperature：</strong> 简单来说，temperature 的参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，大语言模型可能会返回更随机的结果，也就是说这可能会带来更多样化或更具创造性的产出。（调小temperature）实质上，你是在增加其他可能的 token 的权重。在实际应用方面，对于质量保障（QA）等任务，我们可以设置更低的 temperature 值，以促使模型基于事实返回更真实和简洁的结果。 对于诗歌生成或其他创造性任务，适度地调高 temperature 参数值可能会更好。</p>
<p><strong>Top_p：</strong> 同样，使用 top_p（与 temperature 一起称为核采样（nucleus sampling）的技术），可以用来控制模型返回结果的确定性。如果你需要准确和事实的答案，就把参数值调低。如果你在寻找更多样化的响应，可以将其值调高点。</p>
<p>使用Top_P意味着只有词元集合（tokens）中包含top_p概率质量的才会被考虑用于响应，因此较低的Top_p值会选择最有信心的响应。这意味着较高的top_p值将使模型考虑更多可能的词语，包括不太可能的词语，从而导致更多样化的输出。</p>
<p>一般建议是改变 Temperature 和 Top P 其中一个参数就行，<strong>不用两个都调整</strong>。</p>
<p><strong>Max Length：</strong>  可以通过调整 max length 来控制大模型生成的 token 数。指定 Max Length 有助于防止大模型生成冗长或不相关的响应并控制成本。</p>
<p><strong>Stop Sequences：</strong>  stop sequence 是一个字符串，可以阻止模型生成 token，指定 stop sequences 是控制大模型响应长度和结构的另一种方法。例如，您可以通过添加 “11” 作为 stop sequence 来告诉模型生成不超过 10 个项的列表。</p>
<p><strong>Frequency Penalty：</strong> frequency penalty 是对下一个生成的 token 进行惩罚，这个惩罚和 token 在响应和提示中已出现的次数成比例， frequency penalty 越高，某个词再次出现的可能性就越小，这个设置通过给 重复数量多的 Token 设置更高的惩罚来减少响应中单词的重复。</p>
<p><strong>Presence Penalty：</strong> presence penalty 也是对重复的 token 施加惩罚，但与 frequency penalty 不同的是，惩罚对于所有重复 token 都是相同的。出现两次的 token 和出现 10 次的 token 会受到相同的惩罚。 此设置可防止模型在响应中过于频繁地生成重复的词。 如果您希望模型生成多样化或创造性的文本，可以设置更高的 presence penalty，如果您希望模型生成更专注的内容，您可以设置更低的 presence penalty。</p>
<p>与 temperature 和 top_p 一样，一般建议是改变 frequency penalty 和 presence penalty 其中一个参数就行，不要同时调整两个。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM 的发展史]]></title>
        <id>https://panson.top/post/llm-de-fa-zhan-shi/</id>
        <link href="https://panson.top/post/llm-de-fa-zhan-shi/">
        </link>
        <updated>2025-06-21T14:07:06.000Z</updated>
        <content type="html"><![CDATA[<p>大型语言模型(LLM)的发展史是一部从简单到复杂、从专用到通用、从规则驱动到数据驱动的技术进化史。<strong>这一演进过程经历了三个关键阶段：统计语言模型时代、神经网络语言模型时代和基于Transformer的大型语言模型时代</strong>。</p>
<p>每个阶段都带来了对自然语言处理(NLP)能力的质的飞跃。从早期的简单单词预测到如今能够进行复杂推理、多模态交互的通用人工智能，LLM的发展不仅重塑了人机交互方式，也为各行业带来了革命性变革。</p>
<h3 id="一-统计语言模型时代1950s-2010s">一、统计语言模型时代(1950s-2010s)</h3>
<p>统计语言模型是LLM的前身，其核心思想是通过概率统计方法来建模语言的分布规律。<strong>这一阶段的技术特点是基于离散变量表示单词，依赖人工设计的规则和统计方法处理语言</strong>，主要包括 N-gram模型和隐马尔可夫模型(HMM)。</p>
<p>N-gram 模型作为最基础的统计语言模型，最早可以追溯到 20 世纪 50 年代。它通过计算特定n个单词连续出现的概率来预测下一个单词。例如，二元语法模型(Bigram Model)计算的是当前单词和下一个单词的共现概率，三元语法模型(Trigram Model)则考虑当前单词和前两个单词的组合。这种模型结构简单、时间复杂度低，但存在明显的局限性：维度灾难、泛化能力差，以及无法处理一词多义问题  。</p>
<p>随着计算能力的提高和大规模文本语料库的增加，统计方法在 20 世纪 80-90 年代逐渐占据主导地位，特别是在机器翻译领域 。然而，由于统计语言模型学习能力有限，需要学习的词组太多，如果训练集中缺少某些词组，模型的输出概率就会变成0，导致零概率问题。</p>
<p>尽管研究者们通过平滑技术解决这一问题，但依然无法从根本上克服统计语言模型的局限性  。</p>
<p><strong>这一阶段的技术突破包括：</strong></p>
<ul>
<li>1950年：图灵测试提出，成为衡量机器智能的重要标准</li>
<li>1980-1990年代：统计机器翻译(SMT)成为主流，基于 n-gram 模型的系统如 IBM 的 MOSES 开始应用</li>
<li>2001年：Google推出PageRank算法，间接推动了语言模型的语义理解能力</li>
<li>2003年：约书亚·本吉奥提出第一个前馈神经网络语言模型(FFNNLM)，开始尝试将神经网络与语言模型结合</li>
</ul>
<h3 id="二-神经网络语言模型时代2010s">二、神经网络语言模型时代(2010s)</h3>
<p>2010年代，随着深度学习的兴起，基于神经网络的语言模型逐渐取代了统计语言模型。<strong>这一阶段的核心突破是词向量表示和循环神经网络(RNN)的引入，使语言模型能够更好地捕捉语义信息</strong>  。</p>
<p>2013年，Word2Vec和GloVe等词向量表示模型的提出实现了文本语义的分布式向量表示，解决了统计语言模型的离散变量问题。这些模型将单词映射为连续的向量空间，使得计算机能够理解单词之间的语义关系  。然而，这些静态词向量表示模型对单词的词向量表示无法随着上下文语境改变而改变，例如单词&quot;apple&quot;在苹果水果和苹果公司的语境下具有不同的含义，但静态词向量无法区分  。</p>
<p>2014年，Seq2Seq(sequence to sequence)模型被提出，这是基于RNN的模型，首次应用于机器翻译领域  。Seq2Seq模型采用编码器-解码器架构，编码器将源句子编码为特征表示，解码器根据该特征表示生成目标句子。这一模型能够实现完全端到端训练，为生成任务提供了新思路  。然而，Seq2Seq模型主要基于RNN，存在长距离依赖问题，即当输入序列比较长时，模型容易失去对位置靠前字词的记忆。</p>
<p><strong>为解决这一问题，2015年注意力机制(attention)被引入</strong>，用以改进Seq2Seq模型。注意力机制允许模型在处理序列时关注不同的位置，通过权重系数计算出哪些单词之间的关联性更大，提高了模型的可解释性  。这一创新为后续的Transformer架构奠定了基础。</p>
<p>这一阶段的主要技术突破包括：</p>
<ul>
<li>2013年：Word2Vec和GloVe词向量模型发布，开创分布式语义表示</li>
<li>2014年：Seq2Seq模型提出，采用RNN实现端到端序列建模</li>
<li>2015年：注意力机制引入，改善RNN的长距离依赖问题</li>
<li>2016年：LSTM(长短期记忆网络)在机器翻译中的应用，提升模型记忆能力</li>
<li>2017年：Transformer架构提出，彻底解决RNN的长距离依赖问题</li>
</ul>
<h3 id="三-基于transformer的大型语言模型时代2017年至今">三、基于Transformer的大型语言模型时代(2017年至今)</h3>
<p>2017年，Vaswani等人在论文《Attention Is All You Need》中提出Transformer架构，<strong>这一架构完全摒弃了传统的循环和卷积网络，仅依靠自注意力机制来处理序列数据</strong>  。</p>
<p>Transformer的出现是LLM发展的里程碑，它通过并行计算和位置编码，能够高效捕捉长距离的依赖关系，显著提升了模型性能。</p>
<h4 id="1-初期探索阶段2017-2019">1. 初期探索阶段(2017-2019)</h4>
<p>Transformer架构刚提出时，主要用于机器翻译等特定任务  。2018年，Google的高级AI研究员雅各布·德夫林等人在论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》中提出BERT模型，<strong>首次将Transformer应用于双向语言建模，使模型能够同时考虑单词的前后上下文</strong>  。BERT通过掩码语言模型(MLM)和下一句预测(NSP)两个任务进行预训练，显著提升了模型在自然语言理解任务上的表现  。</p>
<p>同年，OpenAI发布了GPT-1模型，基于Transformer的解码器架构，提出&quot;预训练+微调&quot;范式  。GPT-1有1.1亿个参数，通过在大量文本数据上进行预训练，然后针对特定任务进行微调，实现了在多种NLP任务上的出色表现  。</p>
<p>2019年，Google推出了T5模型，<strong>首次将所有NLP任务统一为文本到文本(text-to-text)格式</strong>，使模型能够处理更广泛的任务  。T5采用跨度掩码策略，随机遮蔽输入文本中的连续跨度，然后让模型预测被遮蔽的内容，这一方法进一步提升了模型的生成能力  。</p>
<h4 id="2-参数量爆发阶段2020-2022">2. 参数量爆发阶段(2020-2022)</h4>
<p>2020年，OpenAI发布了GPT-3模型，<strong>拥有1750亿个参数，验证了&quot;大模型+大数据&quot;的缩放定律(Scaling Law)</strong>  。GPT-3通过大规模预训练，实现了强大的少样本学习(few-shot learning)能力，仅通过输入任务描述和少量示例，就能在翻译、问答、文本生成等任务上取得极具竞争力的表现  。</p>
<p>同年，Google推出了PaLM模型，参数规模达到5400亿，强调多步推理能力  。PaLM在多种复杂推理任务上表现出色，超过了人类在BIG-bench基准上的平均水平  。</p>
<p>2021年，Meta发布了LLaMA系列模型，包括7B、13B、33B和65B参数的不同变体，展示了开源大模型的潜力  。LLaMA在多种NLP任务上达到或接近闭源模型的性能，为研究者提供了重要的研究工具  。</p>
<h4 id="3-对齐与优化阶段2022-2023">3. 对齐与优化阶段(2022-2023)</h4>
<p>2022年，OpenAI发布了 ChatGPT 和 InstructGPT 模型，<strong>首次系统应用基于人类反馈的强化学习(RLHF)技术优化模型的指令遵循能力</strong>  。RLHF通过三阶段流程(监督微调、奖励模型训练、PPO优化)使模型输出更符合人类偏好，显著降低了幻觉和有害内容的生成概率  。</p>
<p>同年，Google推出了LaMDA模型，专注于对话应用，进一步提升了模型的对话能力  。LaMDA采用更高效的训练策略，能够在保持高性能的同时降低计算成本  。</p>
<p>2023年，Anthropic发布了Claude模型，强调安全对齐和推理能力。Claude采用不同的对齐方法，如通过对比人类和AI的思考过程来优化模型行为，为LLM的安全应用提供了新思路。</p>
<h4 id="4-多模态与专业化阶段2023-2025">4. 多模态与专业化阶段(2023-2025)</h4>
<p>2023年，OpenAI发布了GPT-4，<strong>首次将文本和图像的多模态信号整合到LLM中</strong>  。GPT-4的多模态能力使模型能够理解并生成图文结合的内容，大大扩展了应用场景  。</p>
<p>2024年，Claude 3系列发布，包括Claude 3.5 Haiku和Sonnet，以及2025年发布的Claude 3.7 Sonnet  。Claude 3.7引入&quot;标准+扩展&quot;双模式，用户可通过API控制&quot;思考预算&quot;，在速度、成本与准确性间灵活权衡  。这一创新使模型能够在不同场景下提供最佳性能。</p>
<p>2025年，LLM的发展呈现出两大趋势：一是多模态能力的深化，从文本+图像扩展到视频、3D生成  ；二是垂直领域的专业化，通过适配微调和技术融合，使模型更好地适应特定行业需求  。</p>
<p><strong>这一阶段的关键技术突破包括：</strong></p>
<ul>
<li>2022年：RLHF技术系统化应用，优化模型指令遵循能力</li>
<li>2023年：GPT-4多模态能力发布，整合文本和图像信号</li>
<li>2023年：LoRA(Low-Rank Adaptation)微调技术提出，显著提升微调效率</li>
<li>2024年：Claude 3.5系列发布，支持PDF解析和跨模态推理</li>
<li>2025年：Claude 3.7 Sonnet引入混合推理模式，支持动态计算资源分配</li>
</ul>
<h3 id="四-llm的训练方法演进">四、LLM的训练方法演进</h3>
<p>LLM的发展不仅体现在模型架构和参数规模上，其训练方法也经历了重要变革。<strong>从早期的简单预训练到如今的复杂多阶段训练流程，LLM的训练方法不断优化，以提高模型性能和降低训练成本</strong>  。</p>
<h4 id="1-预训练技术">1. 预训练技术</h4>
<p>预训练是LLM获取通用语言知识的基础阶段。早期的预训练目标主要是预测下一个单词，如GPT系列模型采用的自回归建模  。BERT则采用了掩码语言模型(MLM)和下一句预测(NSP)两个任务，使模型能够同时考虑单词的前后上下文  。</p>
<p><strong>预训练数据来源也经历了从封闭到开放的转变</strong>  。早期的模型主要使用书籍和特定网站数据，如GPT-1使用了Common Crawl的网页数据和书籍数据。随着模型规模的扩大，数据量也大幅增加，如GPT-3使用了570GB的文本数据  。如今，数据清洗和处理成为预训练的关键环节，如Data-Juicer等系统专门用于大规模文本数据的清洗和预处理  。</p>
<h4 id="2-微调技术">2. 微调技术</h4>
<p>微调是使预训练模型适应特定任务的关键步骤。早期的微调主要是全参数微调，即更新模型的所有参数  。然而，随着模型参数量的增加，全参数微调的成本也大幅提高。</p>
<p><strong>参数效率微调方法的出现显著降低了微调成本</strong>  。2022年，LoRA(Low-Rank Adaptation)技术提出，通过低秩矩阵分解仅更新少量参数，微调效率大幅提升  。2023年，AdaLoRA在LoRA基础上改进，能够自适应选择需要更新的参数层，进一步提升了微调效率  。2024年，LISA(Layerwise Importance Sampled Adam)策略通过分层重要性采样，随机激活少数中间层进行优化，平衡了性能与资源消耗  。</p>
<p>此外，检索增强生成(RAG)技术也被广泛应用于提升LLM的实时性和准确性  。RAG通过结合外部知识库和LLM的能力，使模型能够生成更精确、更即时的回答，有效减少幻觉问题  。</p>
<h4 id="3-对齐技术">3. 对齐技术</h4>
<p>随着LLM能力的增强，如何使模型输出符合人类价值观成为重要问题。<strong>RLHF(基于人类反馈的强化学习)技术的出现为模型对齐提供了有效解决方案</strong>  。</p>
<h2 id=""></h2>
<p>RLHF的三阶段流程包括：监督微调(SFT)、奖励模型(RM)训练和PPO优化  。SFT阶段使模型能够理解指令并生成初步回答；RM阶段通过人工对SFT输出排序，训练能够评估模型输出质量的奖励模型；PPO阶段则使用近端策略优化算法，使模型生成的输出尽可能获得更高的奖励分数，从而更符合人类偏好  。</p>
<p><strong>RLHF技术的改进方向包括：</strong></p>
<ul>
<li>FINE-GRAINED RLHF：将回答拆解为以句子为单位，分别评估事实准确性、相关性和信息完整性</li>
<li>RAFT/RRHF：通过RM对生成模型的输出排序，再使用类似SFT的技术训练选定的样本，减少对PPO的依赖</li>
<li>DPO(Direct Preference Optimization)：直接优化偏好而非依赖RL阶段，大幅减少计算量</li>
</ul>
<h3 id="五-llm的未来发展趋势">五、LLM的未来发展趋势</h3>
<p>展望未来，LLM的发展将沿着多模态深化、垂直领域专业化、轻量化与效率提升、技术融合以及伦理治理等方向演进。</p>
<h4 id="1-多模态能力深化">1. 多模态能力深化</h4>
<p><strong>从文本到图像，再到视频和3D生成，LLM的多模态能力将持续深化</strong>  。目前，GPT-4和Claude 3.5已支持图文结合的内容生成，但视频和3D生成仍处于初级阶段。未来，通过更高效的跨模态注意力机制和数据处理技术，LLM将能够处理更复杂的多模态数据，如视频理解、3D场景生成等  。</p>
<h4 id="2-垂直领域专业化">2. 垂直领域专业化</h4>
<p><strong>通用大模型将向垂直领域专业化发展</strong>  ，通过适配微调和技术融合，使模型更好地适应特定行业需求。</p>
<h2 id="-2"></h2>
<p>例如，在医疗领域，LLM可以结合医学知识图谱和专业数据库，提供更准确的诊断建议和治疗方案；在法律领域，LLM可以学习法律条文和案例，辅助法律研究和文书起草；在金融领域，LLM可以分析市场数据和风险因素，提供投资建议和风险管理方案  。</p>
<h4 id="3-轻量化与效率提升">3. 轻量化与效率提升</h4>
<p><strong>随着模型规模的扩大，轻量化和效率提升将成为重要研究方向</strong>  。混合专家(MoE)架构、参数效率微调方法和专用硬件加速等技术将共同推动LLM的轻量化发展  。例如，Claude 3.7的混合推理模式允许用户根据需求在速度、成本与准确性间灵活权衡  ；专用AI芯片如NVIDIA Hopper将为端侧LLM轻量化提供硬件支持  。</p>
<h4 id="4-量子计算与llm结合">4. 量子计算与LLM结合</h4>
<p><strong>量子计算与LLM的结合将开启新的可能性</strong>。</p>
<p>虽然目前仍处于理论探索阶段，但已有研究表明，LLM可以辅助量子电路设计，如在变分量子特征求解器(VQE)中作为控制器进行经典优化 。同时，量子机器学习框架如MAQA通过量子态制备提升计算效率，为LLM的训练和推理提供新的计算范式  。</p>
<h4 id="5-实时学习与知识更新">5. 实时学习与知识更新</h4>
<p><strong>解决LLM的知识过时问题将成为关键挑战</strong>  。目前，LLM主要依赖预训练和RAG技术来获取最新知识，但这一方法存在检索速度和准确性上的局限性。未来，通过增量学习、在线学习和自适应知识更新等技术，LLM将能够实时吸纳新知识，减少对模型本身已有知识的依赖  。</p>
<h4 id="6-伦理治理与安全应用">6. 伦理治理与安全应用</h4>
<p><strong>随着LLM能力的增强，其伦理治理和安全应用将受到更多关注</strong>  。目前，LLM仍存在非真实性和偏见性输出的问题，如编造学术文献和链接。</p>
<p>未来，通过动态偏见检测、可解释性增强和跨法域适配等技术，LLM的安全性和可靠性将得到提升。同时，多维度评估框架如HELM和IN结构调整也将帮助更好地评估和改进模型性能  。</p>
<h3 id="六-llm发展史的时间线">六、LLM发展史的时间线</h3>
<table>
<thead>
<tr>
<th>年份</th>
<th>关键技术/模型</th>
<th>参数规模</th>
<th>主要贡献</th>
</tr>
</thead>
<tbody>
<tr>
<td>2013</td>
<td>Word2Vec</td>
<td>-</td>
<td>首个分布式词向量模型</td>
</tr>
<tr>
<td>2014</td>
<td>Seq2Seq</td>
<td>-</td>
<td>基于RNN的序列到序列模型</td>
</tr>
<tr>
<td>2015</td>
<td>注意力机制</td>
<td>-</td>
<td>解决RNN的长距离依赖问题</td>
</tr>
<tr>
<td>2017</td>
<td>Transformer</td>
<td>-</td>
<td>引入自注意力机制，实现并行计算</td>
</tr>
<tr>
<td>2018</td>
<td>BERT</td>
<td>3.5B/4B</td>
<td>首个双向Transformer预训练模型</td>
</tr>
<tr>
<td>2018</td>
<td>GPT-1</td>
<td>110M</td>
<td>预训练+微调范式</td>
</tr>
<tr>
<td>2019</td>
<td>GPT-2</td>
<td>1.5B</td>
<td>展示生成文本的可控性挑战</td>
</tr>
<tr>
<td>2019</td>
<td>Megatron-LM</td>
<td>83B</td>
<td>验证模型规模扩展的可行性</td>
</tr>
<tr>
<td>2019</td>
<td>T5</td>
<td>11B/3B</td>
<td>统一文本任务为&quot;文本到文本&quot;格式</td>
</tr>
<tr>
<td>2020</td>
<td>GPT-3</td>
<td>175B</td>
<td>验证少样本学习能力</td>
</tr>
<tr>
<td>2020</td>
<td>PaLM</td>
<td>540B</td>
<td>强调多步推理能力</td>
</tr>
<tr>
<td>2021</td>
<td>LLaMA</td>
<td>7B-65B</td>
<td>开源大模型系列</td>
</tr>
<tr>
<td>2022</td>
<td>InstructGPT</td>
<td>175B</td>
<td>首次系统应用RLHF技术</td>
</tr>
<tr>
<td>2022</td>
<td>ChatGPT</td>
<td>-</td>
<td>人机对话能力突破</td>
</tr>
<tr>
<td>2023</td>
<td>GPT-4</td>
<td>1.8T</td>
<td>多模态能力整合</td>
</tr>
<tr>
<td>2023</td>
<td>Claude 1</td>
<td>-</td>
<td>安全对齐和推理能力</td>
</tr>
<tr>
<td>2023</td>
<td>LoRA</td>
<td>-</td>
<td>参数效率微调技术</td>
</tr>
<tr>
<td>2024</td>
<td>Claude 3.5</td>
<td>-</td>
<td>支持PDF解析和跨模态推理</td>
</tr>
<tr>
<td>2025</td>
<td>Claude 3.7 Sonnet</td>
<td>-</td>
<td>混合推理模式，动态计算资源分配</td>
</tr>
</tbody>
</table>
<h2 id="-3"></h2>
<h3 id="七-llm对社会和行业的影响">七、LLM对社会和行业的影响</h3>
<p>LLM的发展不仅推动了技术进步，也对社会和行业产生了深远影响。<strong>在内容创作领域，LLM大幅提高了内容生成效率，降低了创作门槛</strong>  ；在客户服务领域，LLM驱动的智能客服能够提供24/7的服务，显著提升用户体验  ；在教育领域，LLM可以作为个性化学习助手，帮助学生解决问题和获取知识  ；在医疗领域，LLM可以辅助诊断和治疗方案制定，提高医疗效率  ；在法律领域，LLM可以处理法律文档和研究案例，帮助律师提高工作效率  。</p>
<p>然而，LLM也带来了新的挑战和风险。<strong>模型的非真实性和偏见性输出可能导致虚假信息传播和社会不平等</strong>  ；模型的实时自主学习能力欠缺使得知识更新滞后；模型的强依赖数据集质量和数量也限制了其在特定领域的应用  。未来，随着LLM技术的成熟和应用场景的扩展，这些挑战和风险也将得到更多关注和解决方案。</p>
<h2 id="-4"></h2>
<h3 id="八-结语">八、结语</h3>
<p>从统计语言模型到基于Transformer的大型语言模型，LLM的发展史是一部技术不断突破、能力不断提升的历程。<strong>随着参数规模的扩大、训练方法的优化和多模态能力的深化，LLM正逐步向通用人工智能方向演进</strong>  。然而，这一演进过程也伴随着技术局限和安全风险，需要研究者、开发者和政策制定者共同努力，推动LLM技术的健康发展和广泛应用。</p>
<p>未来，LLM的发展将更加注重轻量化、专业化和安全性，通过与图神经网络、量子计算等技术的融合，进一步拓展其应用边界和能力上限  。<strong>在这一过程中，LLM不仅将重塑人机交互方式，也将成为推动各行业数字化转型和智能化升级的重要力量</strong> 。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[从零开始学 AI 总集篇]]></title>
        <id>https://panson.top/post/ai/</id>
        <link href="https://panson.top/post/ai/">
        </link>
        <updated>2025-06-21T08:28:31.000Z</updated>
        <summary type="html"><![CDATA[<p><em>想多了解一点这个快速发展的世界，学习一下 AI 相关的知识。</em></p>
<h2 id="工具篇">工具篇</h2>
<p>本地部署试过 Ollama + DeepSeek R1 32 b，19 年的英特尔芯片已经带不动了。</p>
]]></summary>
        <content type="html"><![CDATA[<p><em>想多了解一点这个快速发展的世界，学习一下 AI 相关的知识。</em></p>
<h2 id="工具篇">工具篇</h2>
<p>本地部署试过 Ollama + DeepSeek R1 32 b，19 年的英特尔芯片已经带不动了。</p>
<!-- more -->
<p>目前主要用的是 Cherry Studio + 字节方舟的 DeepSeek R1，主要是因为当时领了几十块钱的券~<br>
但是腾讯元宝真的很方便，而且免费。<br>
Web 端的话各种官网混用：腾讯元宝 、ChatGpt、Claude、通义千问。<br>
编辑器的话，用的是Trae，之前用 Trae + Hugo 搭了一个摄影小站：</p>
<ul>
<li>www.timelesslens.site</li>
</ul>
<h2 id="ai-名词">AI 名词</h2>
<h3 id="rag">RAG</h3>
<p>RAG 的全称是：Retrieval-Augmented Generation，翻译成中文是：检索增强生成。</p>
<p>简而言之就是让大语言模型（比如 ChatGPT）在“生成答案”之前，先去找资料（检索）来增强它的知识，再用这些资料来生成更准确的回答。</p>
<h4 id="为什么需要rag">为什么需要RAG？</h4>
<p>因为对于很多大语言模型来说，他的知识是基于历史数据训练出来的，比如GPT-4是截止到2023年的数据，而在这之后发生的所有的新的事件，新的数据，他都是不知道的，那么他的回答就会有这部分的局限性。</p>
<p>还有就是，很多大模型是基于公开的资料训练出来的，而很多私域的信息他是没有学习过的，而很多知识是私有的知识，这就需要通过资料的方式增强他原来不熟悉的知识。</p>
<p>所以，有了RAG之后，就可以基于自己的知识构建自己的知识库，这样就能做到知识的更新和迭代，也能弥补大模型不知道一些特性领域的专业知识的不足。这样就能让大模型的回答更加的准确， 减少幻觉的发生。</p>
<h4 id="如何构建一个rag">如何构建一个RAG？</h4>
<p>1、前置准备</p>
<p>首先我们需要做数据准备，把你要用的资料收集好，比如：公司内部文档（PDF、Word、Markdown）、FAQ列表、产品手册等，然后清洗这些数据，比如去掉无关信息、切分成合理的小段。</p>
<p>然后把每一小段文本用Embedding模型转成向量，把这些向量存到向量数据库里，比如FAISS、Milvus等。</p>
<p>2、检索查询</p>
<p>当用户提问时，先用相同的Embedding模型把问题也转成向量。然后在向量数据库里用向量相似度搜索，找出最相关的几段资料（比如Top 5）。这些找到的内容就是上下文增强材料。</p>
<p>3、生成回答</p>
<p>紧接着，就可以把用户的问题 + 检索到的资料一起，作为Prompt发给大语言模型（LLM）。 这样可以保证模型只在资料范围内生成答案，降低幻觉。</p>
<h3 id="mcp">MCP</h3>
<p>MCP 的全称是 Model Context Protocol，下面是官网上的解释：</p>
<blockquote>
<p>MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.</p>
</blockquote>
<p>MCP全称是模型上下文协议（Model Context Protocol），由Anthropic在2024年11月推出。它本质上是大模型与外部世界连接的标准化接口，就像给大模型装了个万能插座。</p>
<p>MCP中有三个核心组件</p>
<ul>
<li>MCP Hosts：如Claude Desktop或IDE（比如Cursor），作为AI应用的入口，发起数据请求。</li>
<li>MCP Servers：轻量级服务，负责对接具体数据源或工具（如GitHub API、本地文件系统），提供标准化接口。（一般是别人开发好的，你要用的工具）</li>
<li>MCP Clients：协议客户端，维护与服务器的连接并转发请求。</li>
</ul>
<p>有了MCP之后，当用户提出一个问题时，就是大致下面的流程：</p>
<ul>
<li>客户端（Claude Desktop / Cursor）将你的问题发送给大模型（如Claude）。</li>
<li>Claude 分析可用的工具，并决定使用哪一个（或多个）。</li>
<li>客户端通过 MCP Server 执行所选的工具。</li>
<li>工具的执行结果被送回给 Claude。</li>
<li>Claude 结合执行结果构造最终的 prompt 并生成自然语言的回应。</li>
<li>回应最终展示给用户</li>
</ul>
<p>我让 DeepSeek 以生动形象的语言给我解释了一下 MCP，以下是它的回答：</p>
<p>用大白话解释，<strong>大模型中的 MCP 就像一个“万能插座”</strong>，让原本被困在知识牢笼里的 AI 大模型，突然长出了能连接现实世界的手脚。以下是生动解析：</p>
<hr>
<h4 id="1-大模型原本是个孤岛天才">🔌 <strong>1. 大模型原本是个“孤岛天才”</strong></h4>
<p>想象大模型是个满腹经纶的学者，但被关在没有窗户的房间里：</p>
<ul>
<li><strong>知识是旧的</strong>：只能复述训练时学过的内容（比如不知道今天天气）；</li>
<li><strong>无法动手操作</strong>：知道“怎么发邮件”，但没法真的帮你发；</li>
<li><strong>数据隔离</strong>：你电脑里的文件、公司数据库，它都摸不着[citation:1][citation:7]。</li>
</ul>
<blockquote>
<p>💡 就像让厨师背熟菜谱却不给厨房——空有理论，做不出菜！</p>
</blockquote>
<hr>
<h4 id="️-2-mcp-是给大模型配的万能工具箱">🛠️ <strong>2. MCP 是给大模型配的“万能工具箱”</strong></h4>
<p>MCP（Model Context Protocol）是一套<strong>标准化接口协议</strong>，核心功能类似 <strong>“插座+适配器”</strong>：</p>
<ul>
<li><strong>统一接口</strong>：所有外部工具（数据库、天气 API、微信、Excel）只要装上“MCP 插头”（即 MCP 服务器），就能被大模型即插即用[citation:4][citation:6]；</li>
<li><strong>双向通信</strong>：大模型能查数据（如“读销售报表”），也能执行操作（如“订会议室”）[citation:2]；</li>
<li><strong>本地安全防护</strong>：数据不用上传云端，直接在本地处理，像给插座加了保险开关[citation:1][citation:5]。</li>
</ul>
<blockquote>
<p>✅ <strong>举个栗子</strong>：<br>
你说：“把上周销售数据整理成图表，邮件发给老板。”<br>
大模型通过 MCP 自动调用：<br>
1️⃣ <strong>数据库插件</strong> → 拉取数据；<br>
2️⃣ <strong>Python 插件</strong> → 生成图表；<br>
3️⃣ <strong>邮箱插件</strong> → 发送邮件。</p>
</blockquote>
<hr>
<h4 id="️-3-工作流程像ai-指挥官">⚙️ <strong>3. 工作流程像“AI 指挥官”</strong></h4>
<ol>
<li><strong>你下指令</strong>：“导航去公司附近最火的川菜馆。”</li>
<li><strong>大模型分析</strong>：需要调用地图工具 + 实时路况。</li>
<li><strong>MCP 客户端</strong>：找到“百度地图 MCP 服务器”（已装好插头）。</li>
<li><strong>地图服务器</strong>：返回路线、排队时长、评分（数据不离开你手机）。</li>
<li><strong>大模型整合</strong>：“推荐‘蜀香阁’，车程 15 分钟，当前不用排队哦！”。</li>
</ol>
<blockquote>
<p>🌟 <strong>关键突破</strong>：</p>
<ul>
<li><strong>不用重复开发</strong>：每个工具只需一次适配 MCP，所有大模型都能用；</li>
<li><strong>动态扩展能力</strong>：装个“股票插件”，AI 立刻懂炒股；装个“Git 插件”，秒变编程助手[citation:4][citation:5]。</li>
</ul>
</blockquote>
<hr>
<h4 id="4-实际效果ai-从嘴强王者变身全能助理">🚀 <strong>4. 实际效果：AI 从“嘴强王者”变身“全能助理”</strong></h4>
<ul>
<li><strong>查私人数据</strong>：直接问“帮我找昨天写的年终总结 PPT”；</li>
<li><strong>操作跨平台工具</strong>：“订明早 9 点的机票，同步到日历并提醒小李”；</li>
<li><strong>实时响应</strong>：“现在美股苹果股价多少？” → 调用金融数据插件秒回[citation:2][citation:6]。</li>
</ul>
<blockquote>
<p>💡 <strong>一句话总结</strong>：<br>
<strong>MCP 就是大模型的“手脚延伸器”——让 AI 走出虚拟牢笼，用标准化插座连接万物，从此能动手就别吵吵！</strong></p>
</blockquote>
<p>当前百度地图、高德、企业微信等均已支持 MCP，未来你的每一个操作指令，背后可能都是 MCP 在默默调度。</p>
<h3 id="agent">Agent</h3>
<p>todo</p>
<h3 id="springai">SpringAI</h3>
<p>todo</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[G1系列（一）： G1核心参数]]></title>
        <id>https://panson.top/post/g1-xi-lie-yi-g1-he-xin-can-shu/</id>
        <link href="https://panson.top/post/g1-xi-lie-yi-g1-he-xin-can-shu/">
        </link>
        <updated>2025-01-01T06:10:30.000Z</updated>
        <content type="html"><![CDATA[<p>在 G1 GC 中，一些 JVM 参数对性能至关重要。本文将这些参数分为三类，并提供了生产环境下的常用配置建议。</p>
<hr>
<h2 id="一-必须配置的参数">一、必须配置的参数</h2>
<p>这些参数在生产环境中几乎总是需要显式设置，否则 JVM 的默认值可能会不合适。</p>
<h3 id="堆大小设置">堆大小设置</h3>
<p>我们通常使用 <code>-Xms</code> 和 <code>-Xmx</code> 参数来设定 Java 堆的初始大小和最大大小。一个常见的优化建议是将这两个值设为相同，例如 <code>-Xms4g -Xmx4g</code>。这样做可以避免 JVM 在运行时动态调整堆大小，从而减少可能触发的 Full GC。</p>
<p>在容器化部署环境中（如 Docker 和 Kubernetes），JVM 的默认行为可能会自动根据物理内存的百分比来设置堆大小，这有时会带来问题。</p>
<ul>
<li>
<p><strong><code>-XX:InitialRAMPercentage</code></strong>: 当没有显式设置 <code>-Xms</code> 时，JVM 会根据此参数的值来计算初始堆大小。例如，<code>-XX:InitialRAMPercentage=50.0</code> 会将初始堆大小设为容器可用物理内存的 50%。</p>
</li>
<li>
<p><strong><code>-XX:MaxRAMPercentage</code></strong>: 类似地，当没有设置 <code>-Xmx</code> 时，此参数决定了最大堆大小。默认值通常为 80%，这意味着 JVM 可能占用容器 80% 的内存，这在资源紧张的环境中可能会导致问题。</p>
</li>
</ul>
<h3 id="gc-日志">GC 日志</h3>
<p>GC 日志是排查 JVM 问题的关键。建议在生产环境中总是开启，并指定日志文件路径。</p>
<ul>
<li>
<p><strong>JDK 8</strong></p>
<pre><code>-XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/path/gc.log
</code></pre>
</li>
<li>
<p><strong>JDK 9 及更高版本</strong></p>
<pre><code>-Xlog:gc*,gc+heap=debug:file=/path/gc.log:time,uptime,level,tags
</code></pre>
</li>
</ul>
<hr>
<h3 id="元空间metaspace大小">元空间（Metaspace）大小</h3>
<p>元空间用于存储类的元数据。如果应用加载的类非常多，默认的元空间大小可能会导致 <code>java.lang.OutOfMemoryError: Metaspace </code>错误。你可以通过设置 <code>-XX:MaxMetaspaceSize=&lt;size&gt; </code>来限制元空间的最大大小，防止它无限制地占用系统内存。例如，<code>-XX:MaxMetaspaceSize=512m</code> 是一个常见的设置。</p>
<h2 id="二-强烈推荐配置的参数">二、强烈推荐配置的参数</h2>
<p>这些参数可以让你更好地控制应用的延迟和吞吐量。</p>
<ul>
<li>
<p><strong>延迟目标</strong>: 使用 <code>-XX:MaxGCPauseMillis=&lt;milliseconds&gt;</code> 来设置 G1 GC 的最大停顿时间目标。G1 会努力在运行时调整其策略以满足这个目标。默认值为 200 毫秒，但在对延迟要求高的系统中，你可以将其调小，例如设置为 100 毫秒。</p>
</li>
<li>
<p><strong>GC 线程数</strong>: G1 使用并行 GC 线程（<code>ParallelGCThreads</code>）和并发 GC 线程（<code>ConcGCThreads</code>）来执行垃圾回收。通常情况下，JVM 会根据 CPU 核心数自动计算最佳线程数。但在具有大量核心（比如 64 核以上）和巨大内存的机器上，过多的 GC 线程可能会占用过多的 CPU 资源，影响应用程序的正常运行。这时，你可能需要手动调整这些参数，减少线程数量。</p>
</li>
</ul>
<hr>
<h2 id="三-仅在特殊场景下调整的参数">三、仅在特殊场景下调整的参数</h2>
<p>这类参数通常无需配置，除非你遇到特定的性能问题。</p>
<ul>
<li>
<p><strong>晋升阈值</strong>: 参数 <code>-XX:InitiatingHeapOccupancyPercent=&lt;percent&gt;</code> 决定了老年代占用堆空间的百分比。当老年代达到这个阈值时，G1 会触发并发标记周期。默认值为 45%。如果你的应用经常发生 Full GC，你可能需要将这个值调低到 30% 到 40% 之间，以提前触发并发回收，避免 Full GC。</p>
</li>
<li>
<p><strong>大对象（Humongous Object）优化</strong>: G1 将超过 Region 大小一半的对象视为 Humongous 对象。这些对象会直接分配到老年代，可能导致内存碎片。如果你知道应用会产生大量此类大对象，可以尝试调整 <code>-XX:G1HeapRegionSize=&lt;size&gt;</code> 来增加 Region 的大小，以减少碎片并提高效率。</p>
</li>
<li>
<p><strong>防止 Full GC</strong>: 在大内存应用中，如果因为老年代空间不足导致晋升失败，可能会触发 Full GC。<code>G1ReservePercent</code> 参数用于设置 G1 预留的老年代空间百分比，以应对这种突发情况。默认值为 10%。在某些极端场景下，你可以适当调高这个值，比如到 15%。</p>
</li>
</ul>
<hr>
<h2 id="四-总结">四、总结</h2>
<p>对于大多数生产应用，可以重点关注以下参数：</p>
<pre><code class="language-bash">-Xms4g -Xmx4g 
-XX:+UseG1GC 
-XX:MaxGCPauseMillis=200 
-Xlog:gc*:file=/path/gc.log:time,uptime,level,tags
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[自己动手写动态线程池框架 03——基于线程池扩展点增加插件体系]]></title>
        <id>https://panson.top/post/zi-ji-dong-shou-xie-dong-tai-xian-cheng-chi-kuang-jia-03-ji-yu-xian-cheng-chi-kuo-zhan-dian-zeng-jia-cha-jian-ti-xi/</id>
        <link href="https://panson.top/post/zi-ji-dong-shou-xie-dong-tai-xian-cheng-chi-kuang-jia-03-ji-yu-xian-cheng-chi-kuo-zhan-dian-zeng-jia-cha-jian-ti-xi/">
        </link>
        <updated>2024-12-27T06:42:42.000Z</updated>
        <summary type="html"><![CDATA[<p>JDK 线程池的生命周期中包含多个关键扩展点，通过这些扩展点我们可以实现监控、调优、报警等高级功能。</p>
]]></summary>
        <content type="html"><![CDATA[<p>JDK 线程池的生命周期中包含多个关键扩展点，通过这些扩展点我们可以实现监控、调优、报警等高级功能。</p>
<!-- more -->
<h3 id="一-核心扩展点">一、核心扩展点</h3>
<ol>
<li><strong>任务执行监听点</strong>
<ul>
<li><code>beforeExecute()</code>：任务执行前触发</li>
<li><code>afterExecute()</code>：任务完成后触发（含异常情况）</li>
<li>应用场景：耗时统计、超时检测、任务埋点</li>
</ul>
</li>
<li><strong>线程池关闭监听点</strong>
<ul>
<li><code>beforeShutdown()</code>：关闭线程池前触发</li>
<li><code>afterShutdown()</code>：关闭线程池后触发（带未完成任务）</li>
<li><code>afterTerminated()</code>：线程池完全终止后触发</li>
<li>应用场景：资源释放、优雅关机</li>
</ul>
</li>
<li><strong>任务拒绝拦截点</strong>
<ul>
<li><code>beforeRejectedExecution()</code>：触发拒绝策略前执行</li>
<li>应用场景：拒绝计数、预警通知</li>
</ul>
</li>
<li><strong>任务创建/执行拦截点</strong>
<ul>
<li><code>beforeTaskCreate()</code>：创建任务对象前（支持Runnable/Callable）</li>
<li><code>beforeTaskExecute()</code>：任务执行前最后处理点</li>
<li>应用场景：任务装饰、上下文传递</li>
</ul>
</li>
</ol>
<h3 id="二-插件体系设计">二、插件体系设计</h3>
<pre><code class="language-mermaid">classDiagram
    class ThreadPoolPlugin {
        &lt;&lt;Interface&gt;&gt;
        +start() 
        +stop()
    }
    
    class ExecuteAwarePlugin {
        +beforeExecute()
        +afterExecute()
    }
    
    class RejectedAwarePlugin {
        +beforeRejectedExecution()
    }
    
    class ShutdownAwarePlugin {
        +beforeShutdown()
        +afterShutdown()
        +afterTerminated()
    }
    
    class TaskAwarePlugin {
        +beforeTaskCreate()
        +beforeTaskExecute()
    }
    
    ThreadPoolPlugin &lt;|-- ExecuteAwarePlugin
    ThreadPoolPlugin &lt;|-- RejectedAwarePlugin
    ThreadPoolPlugin &lt;|-- ShutdownAwarePlugin
    ThreadPoolPlugin &lt;|-- TaskAwarePlugin
</code></pre>
<pre><code class="language-mermaid">classDiagram
direction BT
class AbstractTaskTimerPlugin {
  + AbstractTaskTimerPlugin() 
  + beforeExecute(Thread, Runnable) void
  # currentTime() long
  # processTaskTime(long) void
  + afterExecute(Runnable, Throwable) void
}
class ExecuteAwarePlugin {
&lt;&lt;Interface&gt;&gt;
  + beforeExecute(Thread, Runnable) void
  + afterExecute(Runnable, Throwable) void
}
class RejectedAwarePlugin {
&lt;&lt;Interface&gt;&gt;
  + beforeRejectedExecution(Runnable, ThreadPoolExecutor) void
}
class ShutdownAwarePlugin {
&lt;&lt;Interface&gt;&gt;
  + afterShutdown(ThreadPoolExecutor, List~Runnable~) void
  + afterTerminated(ExtensibleThreadPoolExecutor) void
  + beforeShutdown(ThreadPoolExecutor) void
}
class Summary {
  + Summary(long, long, long, long) 
  - long taskCount
  - long minTaskTimeMillis
  - long totalTaskTimeMillis
  - long maxTaskTimeMillis
   long maxTaskTimeMillis
   long taskCount
   long minTaskTimeMillis
   long totalTaskTimeMillis
   long avgTaskTimeMillis
}
class TaskAwarePlugin {
&lt;&lt;Interface&gt;&gt;
  + beforeTaskCreate(ThreadPoolExecutor, Runnable, V) Runnable
  + beforeTaskCreate(ThreadPoolExecutor, Callable~V~) Callable~V~
  + beforeTaskExecute(Runnable) Runnable
}
class TaskDecoratorPlugin {
  + TaskDecoratorPlugin() 
  - List~TaskDecorator~ decorators
  + removeDecorator(TaskDecorator) void
  + clearDecorators() void
  + beforeTaskExecute(Runnable) Runnable
  + addDecorator(TaskDecorator) void
   PluginRuntime pluginRuntime
   String id
   List~TaskDecorator~ decorators
}
class TaskRejectCountRecordPlugin {
  + TaskRejectCountRecordPlugin() 
  - AtomicLong rejectCount
  + beforeRejectedExecution(Runnable, ThreadPoolExecutor) void
   PluginRuntime pluginRuntime
   Long rejectCountNum
   AtomicLong rejectCount
   String id
}
class TaskRejectNotifyAlarmPlugin {
  + TaskRejectNotifyAlarmPlugin() 
  + beforeRejectedExecution(Runnable, ThreadPoolExecutor) void
   String id
}
class TaskTimeRecordPlugin {
  + TaskTimeRecordPlugin() 
  + summarize() Summary
  # processTaskTime(long) void
   PluginRuntime pluginRuntime
   String id
}
class TaskTimeoutNotifyAlarmPlugin {
  + TaskTimeoutNotifyAlarmPlugin(String, Long, ThreadPoolExecutor) 
  - Long executeTimeOut
  # processTaskTime(long) void
   String id
   Long executeTimeOut
}
class ThreadPoolExecutorShutdownPlugin {
  + ThreadPoolExecutorShutdownPlugin(long) 
  + long awaitTerminationMillis
  - awaitTerminationIfNecessary(ExtensibleThreadPoolExecutor) void
  + beforeShutdown(ThreadPoolExecutor) void
  # cancelRemainingTask(Runnable) void
  + afterShutdown(ThreadPoolExecutor, List~Runnable~) void
   PluginRuntime pluginRuntime
   String id
   long awaitTerminationMillis
}
class ThreadPoolPlugin {
&lt;&lt;Interface&gt;&gt;
  + start() void
  + stop() void
   PluginRuntime pluginRuntime
   String id
}

AbstractTaskTimerPlugin  ..&gt;  ExecuteAwarePlugin 
ExecuteAwarePlugin  --&gt;  ThreadPoolPlugin 
RejectedAwarePlugin  --&gt;  ThreadPoolPlugin 
ShutdownAwarePlugin  --&gt;  ThreadPoolPlugin 
TaskTimeRecordPlugin  --&gt;  Summary 
TaskAwarePlugin  --&gt;  ThreadPoolPlugin 
TaskDecoratorPlugin  ..&gt;  TaskAwarePlugin 
TaskRejectCountRecordPlugin  ..&gt;  RejectedAwarePlugin 
TaskRejectNotifyAlarmPlugin  ..&gt;  RejectedAwarePlugin 
TaskTimeRecordPlugin  --&gt;  AbstractTaskTimerPlugin 
TaskTimeoutNotifyAlarmPlugin  --&gt;  AbstractTaskTimerPlugin 
ThreadPoolExecutorShutdownPlugin  ..&gt;  ShutdownAwarePlugin 

</code></pre>
<h3 id="三-实用插件实现">三、实用插件实现</h3>
<h4 id="1-监控类插件">1. 监控类插件</h4>
<ul>
<li>
<p><strong>任务耗时统计插件</strong> (TaskTimeRecordPlugin)</p>
<pre><code>public class TaskTimeRecordPlugin extends AbstractTaskTimerPlugin {
    // 记录最小、最大、平均耗时
    public Summary summarize() {
        return new Summary(taskCount, minTaskTime, totalTaskTime, maxTaskTime);
    }
}
</code></pre>
</li>
<li>
<p><strong>拒绝任务计数器</strong> (TaskRejectCountRecordPlugin)</p>
<pre><code>public class TaskRejectCountRecordPlugin implements RejectedAwarePlugin {
    private final AtomicLong rejectCount = new AtomicLong();
    
    public void beforeRejectedExecution(Runnable task, ThreadPoolExecutor executor) {
        rejectCount.incrementAndGet();
    }
}
</code></pre>
</li>
</ul>
<h4 id="2-告警类插件">2. 告警类插件</h4>
<ul>
<li>
<p><strong>任务超时报警</strong> (TaskTimeoutNotifyAlarmPlugin)</p>
<pre><code>public class TaskTimeoutNotifyAlarmPlugin extends AbstractTaskTimerPlugin {
    private final Long executeTimeOut; // 超时阈值
    
    protected void processTaskTime(long taskTime) {
        if(taskTime &gt; executeTimeOut) {
            // 触发告警逻辑
        }
    }
}
</code></pre>
</li>
<li>
<p><strong>拒绝任务告警</strong> (TaskRejectNotifyAlarmPlugin)</p>
<pre><code>public class TaskRejectNotifyAlarmPlugin implements RejectedAwarePlugin {
    public void beforeRejectedExecution(Runnable task, ThreadPoolExecutor executor) {
        // 发送实时告警通知
    }
}
</code></pre>
</li>
</ul>
<h4 id="3-增强类插件">3. 增强类插件</h4>
<ul>
<li>
<p><strong>任务装饰器</strong> (TaskDecoratorPlugin)</p>
<pre><code>public class TaskDecoratorPlugin implements TaskAwarePlugin {
    private final List&lt;TaskDecorator&gt; decorators = new ArrayList&lt;&gt;();
    
    public Runnable beforeTaskExecute(Runnable task) {
        Runnable wrapped = task;
        for(TaskDecorator decorator : decorators) {
            wrapped = decorator.decorate(wrapped);
        }
        return wrapped;
    }
}
</code></pre>
</li>
<li>
<p><strong>优雅停机插件</strong> (ThreadPoolExecutorShutdownPlugin)</p>
<pre><code>public class ThreadPoolExecutorShutdownPlugin implements ShutdownAwarePlugin {
    private final long awaitTerminationMillis;
    
    public void afterShutdown(ThreadPoolExecutor executor, List&lt;Runnable&gt; remainingTasks) {
        // 等待配置时间让任务完成
        executor.awaitTermination(awaitTerminationMillis, TimeUnit.MILLISECONDS);
    }
}
</code></pre>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[自己动手写动态线程池框架 02——自定义可变容量的阻塞队列]]></title>
        <id>https://panson.top/post/zi-ji-dong-shou-xie-dong-tai-xian-cheng-chi-kuang-jia-02-zi-ding-yi-ke-bian-rong-liang-de-zu-sai-dui-lie/</id>
        <link href="https://panson.top/post/zi-ji-dong-shou-xie-dong-tai-xian-cheng-chi-kuang-jia-02-zi-ding-yi-ke-bian-rong-liang-de-zu-sai-dui-lie/">
        </link>
        <updated>2024-12-26T05:38:15.000Z</updated>
        <summary type="html"><![CDATA[<p>考虑到JDK原生阻塞队列的容量不可变性与线程池动态调参需求存在根本性冲突，动态线程池框架需要自定义可变容量的阻塞队列。</p>
]]></summary>
        <content type="html"><![CDATA[<p>考虑到JDK原生阻塞队列的容量不可变性与线程池动态调参需求存在根本性冲突，动态线程池框架需要自定义可变容量的阻塞队列。</p>
<!-- more -->
<hr>
<h3 id="一-原生阻塞队列的致命缺陷静态容量">一、原生阻塞队列的致命缺陷：静态容量</h3>
<p>JDK提供的常用阻塞队列（如<code>ArrayBlockingQueue</code>、<code>LinkedBlockingQueue</code>）均在构造时<strong>固定容量</strong>：</p>
<pre><code>// 容量一旦设定即不可修改
BlockingQueue&lt;Runnable&gt; queue = new ArrayBlockingQueue&lt;&gt;(100); 
</code></pre>
<p>这意味着：</p>
<ol>
<li><strong>无法运行时扩容</strong>：当流量突增时，即使线程池已动态调大<code>maximumPoolSize</code>，任务仍因队列满被拒绝</li>
<li><strong>无法运行时缩容</strong>：低峰期需释放内存时，无法缩小队列占用空间</li>
</ol>
<hr>
<h3 id="二-动态线程池的核心诉求资源弹性">二、动态线程池的核心诉求：资源弹性</h3>
<p>动态线程池的核心价值在于<strong>根据系统负载实时调整资源</strong>：</p>
<pre><code>graph LR
A[监控指标] --&gt;|队列堆积| B(扩容线程数)
A --&gt;|队列持续空| C(缩容线程数)
B --&gt; D{队列满？}
D --&gt;|是| E[需扩容队列容量] 
</code></pre>
<p>此时暴露矛盾：</p>
<ul>
<li><strong>线程数可动态调整</strong>：通过<code>setMaximumPoolSize()</code>实时生效</li>
<li><strong>队列容量仍固定</strong>：成为系统弹性能力的瓶颈</li>
</ul>
<hr>
<h3 id="三-解决方案自定义可变容量队列">三、解决方案：自定义可变容量队列</h3>
<p>通过重写阻塞队列实现<strong>运行时动态调整容量</strong>：</p>
<pre><code>public class ResizableBlockingQueue&lt;T&gt; extends LinkedBlockingQueue&lt;T&gt; {

   // ……省略其他代码，基本与 jdk 父类一致
   
  // 动态更新capacity的方法
  public void setCapacity(int capacity) {
      final int oldCapacity = this.capacity;
      //给capacity成员变量赋值
      this.capacity = capacity;
      final int size = count.get();
      if (capacity &gt; size &amp;&amp; size &gt;= oldCapacity) {
          //因为队列扩容了，所以可以唤醒阻塞的入队线程了
          signalNotFull();
      }
  }

  // 增加唤醒入队线程的方法
  private void signalNotFull() {
      final ReentrantLock putLock = this.putLock;
      putLock.lock();
      try {
          notFull.signal();
      } finally {
          putLock.unlock();
      }
  }

}
</code></pre>
<h4 id="动态调优过程示例">动态调优过程示例：</h4>
<pre><code>// 初始化动态队列（初始容量=50）
ResizableBlockingQueue queue = new ResizableBlockingQueue(50); 

// 监控到队列持续满载时
if (queue.isFull()) {
    // 动态扩容队列（避免触发拒绝策略）
    queue.setCapacity(100); 
    
    // 同步扩容线程数（双维度弹性）
    executor.setMaximumPoolSize(200); 
}
</code></pre>
<hr>
<h3 id="四-自定义队列的核心价值">四、自定义队列的核心价值</h3>
<h4 id="1-突破资源死锁困境">1. <strong>突破资源死锁困境</strong></h4>
<table>
<thead>
<tr>
<th>场景</th>
<th>原生队列</th>
<th>动态队列</th>
</tr>
</thead>
<tbody>
<tr>
<td>突发流量 + 队列满</td>
<td>触发拒绝策略丢任务</td>
<td>即时扩容队列避免任务丢失</td>
</tr>
<tr>
<td>低峰期内存回收</td>
<td>队列持续占用内存</td>
<td>缩容队列释放内存</td>
</tr>
</tbody>
</table>
<h4 id="2-实现精准流量控制">2. <strong>实现精准流量控制</strong></h4>
<pre><code>graph TD
    F[流量探测器] --&gt;|队列使用率&gt;90%| G[队列扩容+线程扩容]
    F --&gt;|队列使用率&lt;30%| H[队列缩容+线程缩容]
    G --&gt; I[避免任务拒绝]
    H --&gt; J[减少内存占用]
</code></pre>
<h4 id="3-保证弹性策略完整性">3. <strong>保证弹性策略完整性</strong></h4>
<blockquote>
<p>线程池弹性 = 线程数弹性 + 队列容量弹性<br>
二者缺一即会导致：</p>
<ul>
<li>仅线程扩容 → 队列满载时新线程无用武之地</li>
<li>仅队列扩容 → 消费者不足导致响应延迟飙升</li>
</ul>
</blockquote>
<hr>
<h3 id="五-生产环境注意事项">五、生产环境注意事项</h3>
<ol>
<li>
<p><strong>容量缩容安全机制</strong></p>
<pre><code>public synchronized void setCapacity(int newCapacity) {
    // 禁止缩容到小于当前元素数
    if (newCapacity &lt; this.size()) {
        throw new IllegalStateException(&quot;Can't reduce below current size&quot;);
    }
    ...
}
</code></pre>
</li>
<li>
<p><strong>避免频繁震荡</strong><br>
增加扩容/缩容的冷却时间（如5分钟内仅允许调整1次）</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[自己动手写动态线程池框架01——监控线程池基础信息]]></title>
        <id>https://panson.top/post/zi-ji-dong-shou-xie-dong-tai-xian-cheng-chi-kuang-jia-01-jian-kong-xian-cheng-chi-ji-chu-xin-xi/</id>
        <link href="https://panson.top/post/zi-ji-dong-shou-xie-dong-tai-xian-cheng-chi-kuang-jia-01-jian-kong-xian-cheng-chi-ji-chu-xin-xi/">
        </link>
        <updated>2024-12-25T04:55:58.000Z</updated>
        <summary type="html"><![CDATA[<p>动态线程池线程池的价值体现在两个维度：</p>
<ol>
<li>状态观测：实时捕获运行时指标，绘制性能趋势；</li>
<li>动态干预：基于流量变化动态调整参数，实现弹性伸缩。</li>
</ol>
<p>如果我们需要实现动态线程池，那我们就需要熟悉线程池的基础信息，了解线程池有哪些基础信息以及如何动态更新这些信息。</p>
]]></summary>
        <content type="html"><![CDATA[<p>动态线程池线程池的价值体现在两个维度：</p>
<ol>
<li>状态观测：实时捕获运行时指标，绘制性能趋势；</li>
<li>动态干预：基于流量变化动态调整参数，实现弹性伸缩。</li>
</ol>
<p>如果我们需要实现动态线程池，那我们就需要熟悉线程池的基础信息，了解线程池有哪些基础信息以及如何动态更新这些信息。</p>
<!-- more -->
<h2 id="一-线程池源码基础信息概览">一、线程池源码基础信息概览</h2>
<p>从线程池源码中可以发现，线程池提供了 get 方法的成员变量，是可以被收集的数据，比如：</p>
<ul>
<li>线程池的核心线程数 corePoolSize</li>
<li>最大线程数 maximumPoolSize</li>
<li>线程池线程的空闲时间 keepAliveTime</li>
<li>核心线程是否允许超时回收 allowCoreThreadTimeOut</li>
<li>线程池的拒绝策略 RejectedExecutionHandler</li>
<li>任务队列 workQueue</li>
<li>线程池当前创建的线程数量 poolSize</li>
<li>曾经创建线程的最大数量 largestPoolSize</li>
<li>当前活跃线程数量 activeCount</li>
<li>线程池的执行的任务总数 taskCount</li>
<li>已经执行完毕的任务总数 completedTaskCount</li>
</ul>
<p>从线程池源码中可以发现：提供了 set 方法的成员变量，是可以被更新的数据，比如：</p>
<ul>
<li>线程池的核心线程数量 corePoolSize</li>
<li>线程池的最大线程数量 maximumPoolSize</li>
<li>线程池的拒绝策略处理器 RejectedExecutionHandler 。</li>
<li>线程池核心线程是否允许超时回收的标志 allowCoreThreadTimeOut</li>
<li>线程池线程的最大空闲时间 keepAliveTime</li>
</ul>
<h2 id="二-信息总结">二、信息总结</h2>
<p>从 <code>ThreadPoolExecutor</code> 源码中提取的核心监控参数：</p>
<table>
<thead>
<tr>
<th><strong>指标类型</strong></th>
<th><strong>参数</strong></th>
<th><strong>源码字段/方法</strong></th>
<th><strong>监控意义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>静态配置</strong></td>
<td><code>corePoolSize</code></td>
<td>核心线程数</td>
<td>系统常驻处理能力基线</td>
</tr>
<tr>
<td></td>
<td><code>maximumPoolSize</code></td>
<td>最大线程数</td>
<td>突发流量承载上限</td>
</tr>
<tr>
<td></td>
<td><code>keepAliveTime</code></td>
<td>空闲线程存活时间</td>
<td>资源回收策略敏感度</td>
</tr>
<tr>
<td></td>
<td><code>allowCoreThreadTimeOut</code></td>
<td>核心线程超时回收</td>
<td>是否允许核心线程闲置退出（true/false）</td>
</tr>
<tr>
<td></td>
<td><code>workQueue</code></td>
<td>任务队列实现类</td>
<td>队列类型（ArrayBlockingQueue, LinkedBlockingQueue等）影响排队策略</td>
</tr>
<tr>
<td><strong>动态运行时</strong></td>
<td><code>poolSize</code></td>
<td><code>getPoolSize()</code></td>
<td><strong>当前存活线程总数</strong>（包含空闲线程）</td>
</tr>
<tr>
<td></td>
<td><code>activeCount</code></td>
<td><code>getActiveCount()</code></td>
<td><strong>正在执行任务的线程数</strong> → 真实并发负载</td>
</tr>
<tr>
<td></td>
<td><code>largestPoolSize</code></td>
<td><code>getLargestPoolSize()</code></td>
<td><strong>历史最大线程数</strong> → 判断线程池扩容峰值需求</td>
</tr>
<tr>
<td></td>
<td><code>taskCount</code></td>
<td><code>getTaskCount()</code></td>
<td><strong>总提交任务数</strong>（包括队列中未执行的任务）</td>
</tr>
<tr>
<td></td>
<td><code>completedTaskCount</code></td>
<td><code>getCompletedTaskCount()</code></td>
<td><strong>已完成任务数</strong> → 结合taskCount计算吞吐量</td>
</tr>
<tr>
<td><strong>拒绝策略</strong></td>
<td><code>RejectedExecutionHandler</code></td>
<td>拒绝策略实例</td>
<td>当队列满且线程达上限时的处理逻辑（AbortPolicy/CallerRunsPolicy等）</td>
</tr>
</tbody>
</table>
<blockquote>
<p>🚨 <strong>备注</strong>：<br>
<code>activeCount / maximumPoolSize &gt; 70%</code> → 提示线程资源紧张；<br>
<code>(taskCount - completedTaskCount) &gt; queueSize</code> → 表明存在任务堆积风险。</p>
</blockquote>
<hr>
<h4 id="三-动态调参运行时可修改的关键参数"><strong>三、动态调参：运行时可修改的关键参数</strong></h4>
<p>线程池支持热更新的参数（通过 <code>setter</code> 方法）：</p>
<pre><code>// 示例：动态调整核心线程数
executor.setCorePoolSize(newCoreSize); 

// 调整最大线程数（触发条件：newMax &gt; current threads）
executor.setMaximumPoolSize(newMaxSize);  

// 允许核心线程超时（适用于低流量时段缩容）
executor.allowCoreThreadTimeOut(true);  

// 调整空闲线程存活时间（单位：纳秒）
executor.setKeepAliveTime(30, TimeUnit.SECONDS);  

// 更换拒绝策略（无需重启）
executor.setRejectedExecutionHandler(new CustomPolicy()); 
</code></pre>
<blockquote>
<p>⚠️ <strong>生产注意事项</strong>：</p>
<ol>
<li>调大 <code>corePoolSize</code> 会<strong>立即创建新线程</strong>，但调小需等待线程超时退出；</li>
<li>修改 <code>maximumPoolSize</code> 时，若新值小于当前线程数，不会强制销毁线程；</li>
<li>动态调参建议配合<strong>监控告警</strong>，避免频繁操作引发震荡。</li>
</ol>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[一次上游更改接口导致的百万级数据扫描引起的宕机问题]]></title>
        <id>https://panson.top/post/yi-ci-shang-you-geng-gai-jie-kou-dao-zhi-de-bai-wan-ji-shu-ju-sao-miao-yin-qi-de-dang-ji-wen-ti-md/</id>
        <link href="https://panson.top/post/yi-ci-shang-you-geng-gai-jie-kou-dao-zhi-de-bai-wan-ji-shu-ju-sao-miao-yin-qi-de-dang-ji-wen-ti-md/">
        </link>
        <updated>2024-11-05T06:25:27.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="一-问题分析过程">一、问题分析过程</h2>
<p>深圳某智能仓现场反馈报错：</p>
]]></summary>
        <content type="html"><![CDATA[<h2 id="一-问题分析过程">一、问题分析过程</h2>
<p>深圳某智能仓现场反馈报错：</p>
<!-- more -->
<figure data-type="image" tabindex="1"><img src="https://github.com/PansonPanson/supply-chain/blob/main/%E4%BE%9B%E5%BA%94%E9%93%BE%E5%AD%90%E7%B3%BB%E7%BB%9F/%E6%99%BA%E8%83%BD%E4%BB%93%E5%82%A8%E7%B3%BB%E7%BB%9F/%E6%8A%80%E6%9C%AF%E7%AF%87/images/%E4%B8%80%E6%AC%A1%E4%B8%8A%E6%B8%B8%E6%9B%B4%E6%94%B9%E6%8E%A5%E5%8F%A3%E5%AF%BC%E8%87%B4%E7%9A%84%E7%99%BE%E4%B8%87%E7%BA%A7%E6%95%B0%E6%8D%AE%E6%89%AB%E6%8F%8F%E5%BC%95%E8%B5%B7%E7%9A%84%E5%AE%95%E6%9C%BA%E9%97%AE%E9%A2%98/1749622243260-be432462-445d-431e-8040-8730e79a9d51.png?raw=true" alt="img" loading="lazy"></figure>
<p>接着全场工作站页面无法点击。</p>
<p>立即连接该仓库的服务器，查看 WES 服务 Docker 容器的状态，是 unhealthy 状态。考虑恢复优先，立即重启容器。现场恢复，但过一会儿，现场又反馈很卡顿。重新连线，发现容器状态又变成 unhealthy 了。</p>
<p>执行 <code>jstat -gc pid 1000</code>：</p>
<ul>
<li>最近有 Full GC 过。</li>
<li>GC 很频繁。</li>
</ul>
<p>而且慢 SQL 很多，一次三十秒以上的有很多。从慢 SQL 语句内容上分析：</p>
<ul>
<li>查询的表内容主要集中在出库单明细、出库单实操明细、反馈明细上。</li>
<li>查询条件都是 <code>in (id1, id2……)</code>,并且都是大列表查询。</li>
<li>状态都与待反馈或者反馈中有关。</li>
</ul>
<p><strong>从业务关联上猜测是发货相关逻辑出现了问题。</strong></p>
<p>查看出库单状态：</p>
<pre><code class="language-json">select count(*) from 出库单表 where state = 待发货状态
</code></pre>
<ul>
<li>好家伙，两百多万条全是待发货。</li>
<li>再看反馈明细，未反馈的实操明细更多，600 多万……</li>
</ul>
<p><strong>已经有了初步怀疑，反馈逻辑出问题了。</strong></p>
<p><strong>扒代码，流程大概如下:</strong></p>
<ul>
<li>
<p>WES 发送 MQ 消息，告知单据已拣选完成，需要反馈上游告知单据的拣选信息。</p>
</li>
<li>
<p>反馈服务收到消息后，请求上游告知出库单拣选信息。</p>
</li>
<li>
<p>收到上游接口成功的 response 后，调用接口，处理一些逻辑，最后更新单据反馈明细为反馈完成。</p>
</li>
<li>
<p>WES 定时器逻辑：</p>
</li>
<li>
<p>WES 定时器查询订单状态为待发货并且反馈状态为反馈中的订单。</p>
</li>
<li>
<p>根据订单查询反馈明细。</p>
</li>
<li>
<p>如果反馈明细都反馈完成了，则更新订单反馈状态为反馈完成。</p>
</li>
</ul>
<p>了解上述简要逻辑之后，我们再来分析现场问题。</p>
<p>查看反馈日志，捞了一些待发货的单子，发现所有的单子都反馈了。理论上对于上游来说，所有的流程已经走完了。</p>
<p><strong>那只能是最后一步更新单据反馈明细状态出了问题</strong>：更新单据反馈明细为反馈完成。</p>
<p><strong>看 log</strong>，很多 <code>response error</code>， 但是实际的 http status code 是 200。感觉不太对劲，应该是最关键的信息了。</p>
<p>细看日志上下文，发现上游返回的 response 格式不对，理论上按约定是：</p>
<pre><code class="language-json">{
  &quot;一些额外信息&quot;: &quot;&quot;,
  &quot;body&quot;: {
    &quot;code&quot;: &quot;200&quot;,
    &quot;data&quot;: &quot;……&quot;,
    &quot;message&quot;: &quot;success&quot;,
    &quot;success&quot;: false
  }
}
</code></pre>
<p>但上游实际返回的是：</p>
<pre><code class="language-json">{
  &quot;success&quot;: true,
  &quot;code&quot;: 0,
  &quot;message&quot;: &quot;success&quot;,
  &quot;data&quot;: &quot;……&quot;,
}
</code></pre>
<p>导致反馈服务根据约定格式解析的时候发现获取不到 body 信息，以为出错了。</p>
<p><strong>收到上游接口成功的 response 后，调用接口，处理一些逻辑，最后更新单据反馈明细为反馈完成。</strong><br>
所以单据反馈明细一直是反馈中的状态，导致后续的逻辑一直无法正常执行。</p>
<h2 id="二-故障原因总结">二、故障原因总结</h2>
<p>上游在升级的时候，误改了接口格式，导致 WES 按照约定的格式处理接口返回数据时，拿不到指定格式的数据，导致单据无法正常完结。</p>
<h2 id="三-如何快速恢复">三、如何快速恢复</h2>
<ul>
<li><strong>写 SQL 批量更新单据状态为完成，并打上特殊标记：</strong> 先避免重启一直扫表查大量数据问题，让现场恢复。</li>
<li><strong>重启线上服务。</strong></li>
</ul>
<p><strong>代码层面：</strong></p>
<ul>
<li>
<p><strong>增量数据：</strong></p>
<ul>
<li>
<p>兼容上游数据格式。</p>
</li>
<li>
<p>与上游再次沟通，约定不要随意更改接口格式。</p>
</li>
</ul>
</li>
<li>
<p><strong>存量数据</strong>更改定时器逻辑：</p>
<ul>
<li>
<p>记录最大 ID。</p>
</li>
<li>
<p>每次根据状态加特殊标记捞取 1000 条数据。</p>
</li>
<li>
<p>批量执行：收到上游接口成功的 response 后，调用接口，处理一些逻辑，最后更新单据反馈明细为反馈完成。</p>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
</feed>