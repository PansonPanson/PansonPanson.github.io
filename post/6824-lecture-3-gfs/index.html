<html>
<head>
    <meta charset="utf-8"/>
<meta name="description" content=""/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>

<title>6.824 Lecture 3: GFS | Panson</title>

<link rel="shortcut icon" href="https://panson.top/favicon.ico?v=1766826271561">

<link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://panson.top/styles/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css">

<script src="https://cdn.jsdelivr.net/npm/@highlightjs/cdn-assets/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dockerfile.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dart.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/moment@2.27.0/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/js/bootstrap.min.js"></script>
<!-- DEMO JS -->
<!--<script src="media/scripts/index.js"></script>-->



    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.css">
</head>
<body>
<div class="main gt-bg-theme-color-first">
    <nav class="navbar navbar-expand-lg">
    <div class="navbar-brand">
        <img class="user-avatar" src="/images/avatar.png" alt="头像">
        <div class="site-name gt-c-content-color-first">
            Panson
        </div>
    </div>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <i class="fas fa-bars gt-c-content-color-first" style="font-size: 18px"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <div class="navbar-nav mr-auto" style="text-align: center">
            
                <div class="nav-item">
                    
                        <a href="/" class="menu gt-a-link">
                            home
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/archives" class="menu gt-a-link">
                            archives
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/post/sourcecodereading" class="menu gt-a-link">
                            源码阅读与造轮子
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/post/ai" class="menu gt-a-link">
                            AI
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/post/supply-chain" class="menu gt-a-link">
                            供应链
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/post/distributed-job-scheduler-system" class="menu gt-a-link">
                            任务调度
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/post/photograph" class="menu gt-a-link">
                            摄影
                        </a>
                    
                </div>
            
                <div class="nav-item">
                    
                        <a href="/post/about" class="menu gt-a-link">
                            about
                        </a>
                    
                </div>
            
        </div>
        <div style="text-align: center">
            <form id="gridea-search-form" style="position: relative" data-update="1766826271561" action="/search/index.html">
                <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
                <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
        </div>
    </div>
</nav>

    <div class="post-container">
        <div class="post-detail gt-bg-theme-color-second">
            <article class="gt-post-content">
                <h2 class="post-title">
                    6.824 Lecture 3: GFS
                </h2>
                <div class="post-info">
                    <time class="post-time gt-c-content-color-first">
                        · 2022-08-15 ·
                    </time>
                    
                </div>
                <div class="post-content">
                    <p>The Google File System<br>
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung<br>
SOSP 2003</p>
<p>Why are we reading this paper?<br>
distributed storage is a key abstraction<br>
what should the interface/semantics look like?<br>
how should it work internally?<br>
GFS paper touches on many themes of 6.824<br>
parallel performance, fault tolerance, replication, consistency<br>
good systems paper -- details from apps all the way to network<br>
successful real-world design</p>
<p>Why is distributed storage hard?<br>
high performance -&gt; shard data over many servers<br>
many servers -&gt; constant faults<br>
fault tolerance -&gt; replication<br>
replication -&gt; potential inconsistencies<br>
better consistency -&gt; low performance</p>
<p>What would we like for consistency?<br>
Ideal model: same behavior as a single server<br>
server uses disk storage<br>
server executes client operations one at a time (even if concurrent)<br>
reads reflect previous writes<br>
even if server crashes and restarts<br>
thus:<br>
suppose C1 and C2 write concurrently, and after the writes have<br>
completed, C3 and C4 read. what can they see?<br>
C1: Wx1<br>
C2: Wx2<br>
C3:     Rx?<br>
C4:         Rx?<br>
answer: either 1 or 2, but both have to see the same value.<br>
This is a &quot;strong&quot; consistency model.<br>
But a single server has poor fault-tolerance.</p>
<p>Replication for fault-tolerance makes strong consistency tricky.<br>
a simple but broken replication scheme:<br>
two replica servers, S1 and S2<br>
clients send writes to both, in parallel<br>
clients send reads to either<br>
in our example, C1's and C2's write messages could arrive in<br>
different orders at the two replicas<br>
if C3 reads S1, it might see x=1<br>
if C4 reads S2, it might see x=2<br>
or what if S1 receives a write, but<br>
the client crashes before sending the write to S2?<br>
that's not strong consistency!<br>
better consistency usually requires communication to<br>
ensure the replicas stay in sync -- can be slow!<br>
lots of tradeoffs possible between performance and consistency<br>
we'll see one today</p>
<p>GFS</p>
<p>Context:<br>
Many Google services needed a big fast unified storage system<br>
Mapreduce, crawler/indexer, log storage/analysis, Youtube (?)<br>
Global (over a single data center): any client can read any file<br>
Allows sharing of data among applications<br>
Automatic &quot;sharding&quot; of each file over many servers/disks<br>
For parallel performance<br>
To increase space available<br>
Automatic recovery from failures<br>
Just one data center per deployment<br>
Just Google applications/users<br>
Aimed at sequential access to huge files; read or append<br>
I.e. not a low-latency DB for small items</p>
<p>What was new about this in 2003? How did they get an SOSP paper accepted?<br>
Not the basic ideas of distribution, sharding, fault-tolerance.<br>
Huge scale.<br>
Used in industry, real-world experience.<br>
Successful use of weak consistency.<br>
Successful use of single master.</p>
<p>Overall structure<br>
clients (library, RPC -- but not visible as a UNIX FS)<br>
each file split into independent 64 MB chunks<br>
chunk servers, each chunk replicated on 3<br>
every file's chunks are spread over the chunk servers<br>
for parallel read/write (e.g. MapReduce), and to allow huge files<br>
single master (!), and master replicas<br>
division of work: master deals w/ naming, chunkservers w/ data</p>
<p>Master state<br>
in RAM (for speed, must be smallish):<br>
file name -&gt; array of chunk handles (nv)<br>
chunk handle -&gt; version # (nv)<br>
list of chunkservers (v)<br>
primary (v)<br>
lease time (v)<br>
on disk:<br>
log<br>
checkpoint</p>
<p>Why a log? and checkpoint?</p>
<p>Why big chunks?</p>
<p>What are the steps when client C wants to read a file?</p>
<ol>
<li>C sends filename and offset to master M (if not cached)</li>
<li>M finds chunk handle for that offset</li>
<li>M replies with list of chunkservers<br>
only those with latest version</li>
<li>C caches handle + chunkserver list</li>
<li>C sends request to nearest chunkserver<br>
chunk handle, offset</li>
<li>chunk server reads from chunk file on disk, returns</li>
</ol>
<p>How does the master know what chunkservers have a given chunk?</p>
<p>What are the steps when C wants to do a &quot;record append&quot;?<br>
paper's Figure 2</p>
<ol>
<li>C asks M about file's last chunk</li>
<li>if M sees chunk has no primary (or lease expired):<br>
2a. if no chunkservers w/ latest version #, error<br>
2b. pick primary P and secondaries from those w/ latest version #<br>
2c. increment version #, write to log on disk<br>
2d. tell P and secondaries who they are, and new version #<br>
2e. replicas write new version # to disk</li>
<li>M tells C the primary and secondaries</li>
<li>C sends data to all (just temporary...), waits</li>
<li>C tells P to append</li>
<li>P checks that lease hasn't expired, and chunk has space</li>
<li>P picks an offset (at end of chunk)</li>
<li>P writes chunk file (a Linux file)</li>
<li>P tells each secondary the offset, tells to append to chunk file</li>
<li>P waits for all secondaries to reply, or timeout<br>
secondary can reply &quot;error&quot; e.g. out of disk space</li>
<li>P tells C &quot;ok&quot; or &quot;error&quot;</li>
<li>C retries from start if error</li>
</ol>
<p>What consistency guarantees does GFS provide to clients?<br>
Needs to be in a form that tells applications how to use GFS.</p>
<p>Here's a possibility:</p>
<p>If the primary tells a client that a record append succeeded, then<br>
any reader that subsequently opens the file and scans it will see<br>
the appended record somewhere.</p>
<p>(But not that failed appends won't be visible, or that all readers<br>
will see the same file content, or the same order of records.)</p>
<p>How can we think about how GFS fulfils this guarantee?<br>
Look at its handling of various failures:<br>
crash, crash+reboot, crash+replacement, message loss, partition.<br>
Ask how GFS ensures critical properties.</p>
<ul>
<li>
<p>What if an appending client fails at an awkward moment?<br>
Is there an awkward moment?</p>
</li>
<li>
<p>What if the appending client has cached a stale (wrong) primary?</p>
</li>
<li>
<p>What if the reading client has cached a stale secondary list?</p>
</li>
<li>
<p>Could a master crash+reboot cause it to forget about the file?<br>
Or forget what chunkservers hold the relevant chunk?</p>
</li>
<li>
<p>Two clients do record append at exactly the same time.<br>
Will they overwrite each others' records?</p>
</li>
<li>
<p>Suppose one secondary never hears the append command from the primary.<br>
What if reading client reads from that secondary?</p>
</li>
<li>
<p>What if the primary crashes before sending append to all secondaries?<br>
Could a secondary that <em>didn't</em> see the append be chosen as the new primary?</p>
</li>
<li>
<p>Chunkserver S4 with an old stale copy of chunk is offline.<br>
Primary and all live secondaries crash.<br>
S4 comes back to life (before primary and secondaries).<br>
Will master choose S4 (with stale chunk) as primary?<br>
Better to have primary with stale data, or no replicas at all?</p>
</li>
<li>
<p>What should a primary do if a secondary always fails writes?<br>
e.g. dead, or out of disk space, or disk has broken.<br>
Should the primary drop secondary from set of secondaries?<br>
And then return success to client appends?<br>
Or should the primary keep sending ops, and having them fail,<br>
and thus fail every client write request?</p>
</li>
<li>
<p>What if primary S1 is alive and serving client requests,<br>
but network between master and S1 fails?<br>
&quot;network partition&quot;<br>
Will the master pick a new primary?<br>
Will there now be two primaries?<br>
So that the append goes to one primary, and the read to the other?<br>
Thus breaking the consistency guarantee?<br>
&quot;split brain&quot;</p>
</li>
<li>
<p>If there's a partitioned primary serving client appends, and its<br>
lease expires, and the master picks a new primary, will the new<br>
primary have the latest data as updated by partitioned primary?</p>
</li>
<li>
<p>What if the master fails altogether.<br>
Will the replacement know everything the dead master knew?<br>
E.g. each chunk's version number? primary? lease expiry time?</p>
</li>
<li>
<p>Who/what decides the master is dead, and must be replaced?<br>
Could the master replicas ping the master, take over if no response?</p>
</li>
<li>
<p>What happens if the entire building suffers a power failure?<br>
And then power is restored, and all servers reboot.</p>
</li>
<li>
<p>Suppose the master wants to create a new chunk replica.<br>
Maybe because too few replicas.<br>
Suppose it's the last chunk in the file, and being appended to.<br>
How does the new replica ensure it doesn't miss any appends?<br>
After all it is not yet one of the secondaries.</p>
</li>
<li>
<p>Is there <em>any</em> circumstance in which GFS will break the guarantee?<br>
i.e. append succeeds, but subsequent readers don't see the record.<br>
All master replicas permanently lose state (permanent disk failure).<br>
Could be worse: result will be &quot;no answer&quot;, not &quot;incorrect data&quot;.<br>
&quot;fail-stop&quot;<br>
All chunkservers holding the chunk permanently lose disk content.<br>
again, fail-stop; not the worse possible outcome<br>
CPU, RAM, network, or disk yields an incorrect value.<br>
checksum catches some cases, but not all<br>
Time is not properly synchronized, so leases don't work out.<br>
So multiple primaries, maybe write goes to one, read to the other.</p>
</li>
</ul>
<p>What application-visible anomalous behavior does GFS allow?<br>
Will all clients see the same file content?<br>
Could one client see a record that another client doesn't see at all?<br>
Will a client see the same content if it reads a file twice?<br>
Will all clients see successfully appended records in the same order?</p>
<p>Will these anomalies cause trouble for applications?<br>
How about MapReduce?</p>
<p>What would it take to have no anomalies -- strict consistency?<br>
I.e. all clients see the same file content.<br>
Too hard to give a real answer, but here are some issues.</p>
<ul>
<li>Primary should detect duplicate client write requests.<br>
Or client should not issue them.</li>
<li>All secondaries should complete each write, or none.<br>
Perhaps tentative writes until all promise to complete it?<br>
Don't expose writes until all have agreed to perform them!</li>
<li>If primary crashes, some replicas may be missing the last few ops.<br>
New primary must talk to all replicas to find all recent ops,<br>
and sync with secondaries.</li>
<li>To avoid client reading from stale ex-secondary, either all client<br>
reads must go to primary, or secondaries must also have leases.<br>
You'll see all this in Labs 2 and 3!</li>
</ul>
<p>Performance (Figure 3)<br>
large aggregate throughput for read (3 copies, striping)<br>
94 MB/sec total for 16 chunkservers<br>
or 6 MB/second per chunkserver<br>
is that good?<br>
one disk sequential throughput was about 30 MB/s<br>
one NIC was about 10 MB/s<br>
Close to saturating network (inter-switch link)<br>
So: individual server performance is low<br>
but scalability is good<br>
which is more important?<br>
Table 3 reports 500 MB/sec for production GFS, which is a lot<br>
writes to different files lower than possible maximum<br>
authors blame their network stack (but no detail)<br>
concurrent appends to single file<br>
limited by the server that stores last chunk<br>
hard to interpret after 15 years, e.g. how fast were the disks?</p>
<p>Random issues worth considering<br>
What would it take to support small files well?<br>
What would it take to support billions of files?<br>
Could GFS be used as wide-area file system?<br>
With replicas in different cities?<br>
All replicas in one datacenter is not very fault tolerant!<br>
How long does GFS take to recover from a failure?<br>
Of a primary/secondary?<br>
Of the master?<br>
How well does GFS cope with slow chunkservers?</p>
<p>Retrospective interview with GFS engineer:<br>
http://queue.acm.org/detail.cfm?id=1594206<br>
file count was the biggest problem<br>
eventual numbers grew to 1000x those in Table 2 !<br>
hard to fit in master RAM<br>
master scanning of all files/chunks for GC is slow<br>
1000s of clients too much CPU load on master<br>
applications had to be designed to cope with GFS semantics<br>
and limitations<br>
master fail-over initially entirely manual, 10s of minutes<br>
BigTable is one answer to many-small-files problem<br>
and Colossus apparently shards master data over many masters</p>
<p>Summary<br>
case study of performance, fault-tolerance, consistency<br>
specialized for MapReduce applications<br>
good ideas:<br>
global cluster file system as universal infrastructure<br>
separation of naming (master) from storage (chunkserver)<br>
sharding for parallel throughput<br>
huge files/chunks to reduce overheads<br>
primary to sequence writes<br>
leases to prevent split-brain chunkserver primaries<br>
not so great:<br>
single master performance<br>
ran out of RAM and CPU<br>
chunkservers not very efficient for small files<br>
lack of automatic fail-over to master replica<br>
maybe consistency was too relaxed</p>

                </div>
            </article>
        </div>

        

        

        

        

        <div class="site-footer gt-c-content-color-first">
    <div class="slogan gt-c-content-color-first">清风拂山岗，明月照大江。</div>
    <div class="social-container">
        
            
        
            
        
            
        
            
        
            
        
            
        
    </div>
    <div class="footer-info">
        Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
    </div>
    <div>
        Theme by <a href="https://imhanjie.com/" target="_blank">imhanjie</a>, Powered by <a
                href="https://github.com/getgridea/gridea" target="_blank">Gridea | <a href="https://panson.top/atom.xml" target="_blank">RSS</a></a>
    </div>
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

    </div>
</div>
</body>
</html>
